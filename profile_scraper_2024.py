import pandas as pd
import sys
import os
import time
import re
import argparse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import random
import traceback

# Configuration de l'encodage pour Windows
if sys.platform.startswith('win'):
    try:
        # Forcer l'encodage UTF-8 pour la console Windows
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        # Fallback pour les anciennes versions de Python
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

def safe_print(message):
    """Fonction d'impression s√©curis√©e qui g√®re les probl√®mes d'encodage"""
    try:
        print(message)
    except UnicodeEncodeError:
        # Remplacer les √©mojis par des caract√®res ASCII si probl√®me d'encodage
        safe_message = message.encode('ascii', 'replace').decode('ascii')
        print(safe_message)

class ProfileScraper:
    def __init__(self, excel_file=None, slow_mode=False):
        self.data = None
        self.driver = None
        self.slow_mode = slow_mode
        if excel_file and os.path.exists(excel_file):
            try:
                self.data = pd.read_excel(excel_file)
                print(f"Donn√©es charg√©es depuis {excel_file}")
                print(f"Colonnes: {list(self.data.columns)}")
                print(f"Nombre d'entr√©es: {len(self.data)}")
            except Exception as e:
                print(f"Erreur lors de l'ouverture du fichier Excel: {str(e)}")
        else:
            # Cr√©er un nouveau DataFrame avec les colonnes simplifi√©es
            self.data = pd.DataFrame(columns=[
                'Intitul√© de poste', 
                'Pr√©nom', 
                'Nom', 
                'Entreprise', 
                'LinkedIn',
                'Date d\'ajout',
                'Notes'
            ])
            print("Nouveau fichier de donn√©es cr√©√©")
    
    def setup_browser(self):
        """Configure le navigateur pour le scraping avec techniques anti-CAPTCHA"""
        chrome_options = Options()
        
        # Techniques anti-d√©tection pour √©viter les CAPTCHAs
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # User agent r√©aliste
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        ]
        chrome_options.add_argument(f"--user-agent={random.choice(user_agents)}")
        
        # Autres options pour para√Ætre plus humain
        chrome_options.add_argument("--window-size=1366,768")  # Taille d'√©cran commune
        chrome_options.add_argument("--disable-notifications")
        chrome_options.add_argument("--disable-popup-blocking")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--allow-running-insecure-content")
        
        # Mode sans interface (d√©commentez si n√©cessaire)
        # chrome_options.add_argument("--headless=new")  # Nouveau mode headless Chrome
        
        try:
            print("üîß Initialisation du navigateur avec techniques anti-d√©tection...")
            
            # Trouver Chrome
            chrome_paths = [
                "C:/Program Files/Google/Chrome/Application/chrome.exe",
                "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
            ]
            
            if os.environ.get("LOCALAPPDATA"):
                chrome_paths.append(os.environ.get("LOCALAPPDATA") + "/Google/Chrome/Application/chrome.exe")
            
            chrome_path = None
            for path in chrome_paths:
                if os.path.exists(path):
                    chrome_path = path
                    print(f"‚úì Chrome trouv√© √†: {path}")
                    break
            
            if chrome_path:
                chrome_options.binary_location = chrome_path
                self.driver = webdriver.Chrome(options=chrome_options)
                
                # Script pour masquer l'automatisation
                self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                
                print("‚úÖ Navigateur initialis√© avec succ√®s (mode anti-d√©tection)")
                return True
            else:
                print("‚ùå Chrome n'a pas √©t√© trouv√© dans les emplacements standards")
                return False
                
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation du navigateur: {str(e)}")
            print("üîÑ Tentative de basculement en mode simulation...")
            return False
    
    def search_linkedin_with_delays(self, job_title, num_results=5):
        """Recherche LinkedIn avec d√©lais humains pour √©viter les CAPTCHAs"""
        if not self.driver:
            if not self.setup_browser():
                print("‚ö†Ô∏è  Navigateur non disponible, la recherche ne peut pas continuer")
                return []
        
        results = []
        search_query = f"{job_title}"
        
        # D√©terminer les d√©lais en fonction du mode
        base_delay = 5 if self.slow_mode else 3
        search_delay = 8 if self.slow_mode else 4
        load_delay = 10 if self.slow_mode else 5
        
        try:
            print(f"üîç Recherche Google pour: {search_query}")
            
            # Aller d'abord sur Google pour √©tablir une session normale
            print("üìÑ Chargement de Google...")
            self.driver.get("https://www.google.com")
            time.sleep(base_delay)  # D√©lai humain
            
            # G√©rer le consentement aux cookies si n√©cessaire
            try:
                consent_buttons = [
                    "//button[contains(., 'Tout accepter')]",
                    "//button[contains(., 'Accept all')]", 
                    "//button[contains(., 'J\\'accepte')]"
                ]
                for button_xpath in consent_buttons:
                    try:
                        consent_button = WebDriverWait(self.driver, 3).until(
                            EC.element_to_be_clickable((By.XPATH, button_xpath))
                        )
                        consent_button.click()
                        print("‚úì Consentement cookies accept√©")
                        time.sleep(2)
                        break
                    except:
                        continue
            except:
                print("‚Ñπ  Pas de popup de consentement")
            
            # Chercher la barre de recherche et taper comme un humain
            try:
                search_box = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.NAME, "q"))
                )
                
                # Effacer et taper lentement comme un humain
                search_box.clear()
                search_text = f"site:linkedin.com/in/ {search_query}"
                
                for char in search_text:
                    search_box.send_keys(char)
                    time.sleep(0.1 + (0.05 * __import__('random').random()))  # D√©lai de frappe humain
                
                print(f"üî§ Recherche tap√©e: {search_text}")
                time.sleep(1)
                
                # Appuyer sur Entr√©e
                search_box.send_keys('\n')
                time.sleep(4)  # Attendre les r√©sultats
                
            except Exception as e:
                print(f"‚ùå Erreur lors de la saisie: {str(e)}")
                print("üîÑ Tentative avec URL directe...")
                encoded_query = search_query.replace(' ', '+')
                self.driver.get(f"https://www.google.com/search?q=site:linkedin.com/in/+{encoded_query}")
                time.sleep(5)
            
            # V√©rifier s'il y a un CAPTCHA
            if "captcha" in self.driver.page_source.lower() or "unusual traffic" in self.driver.page_source.lower():
                print("üö´ CAPTCHA d√©tect√© ! La recherche ne peut pas continuer.")
                return []
            
            print("üìä Analyse des r√©sultats...")
            
            # Capture d'√©cran pour d√©bogage si n√©cessaire
            screenshot_path = f"search_results_{job_title.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            self.driver.save_screenshot(screenshot_path)
            print(f"üì∏ Capture d'√©cran enregistr√©e: {screenshot_path}")
            
            # D√©tecter et explorer les r√©sultats avec une approche multistrat√©gie
            found_linkedin_links = self._extract_linkedin_links()
            
            if not found_linkedin_links:
                print("‚ö†Ô∏è  Aucun lien LinkedIn trouv√© dans les r√©sultats.")
                return []
            
            print(f"‚úì {len(found_linkedin_links)} liens LinkedIn trouv√©s")
            
            count = 0
            for i, link_info in enumerate(found_linkedin_links):
                if count >= num_results:
                    break
                
                try:
                    print(f"  üìÑ Analyse du profil {i+1}...")
                    
                    # Extraire les informations en utilisant l'√©l√©ment parent complet
                    profile_info = self._extract_profile_from_result(link_info, job_title)
                    
                    # Ne pas ajouter de profil si les informations sont trop incompl√®tes
                    if not profile_info.get('Pr√©nom') or not profile_info.get('Nom') or not profile_info.get('Entreprise'):
                        safe_print(f"   ‚ùå Profil ignor√© car informations incompl√®tes: {link_info.get('url', '')}")
                        continue  # Passer au profil suivant
                    
                    results.append(profile_info)
                    count += 1
                    print(f"    ‚úì Profil extrait: {profile_info['Pr√©nom']} {profile_info['Nom']} chez {profile_info['Entreprise']}")
                    
                    # D√©lai entre chaque extraction
                    time.sleep(1 + (0.5 * __import__('random').random()))
                    
                except Exception as e:
                    print(f"    ‚ö†Ô∏è  Erreur lors de l'extraction du profil {i+1}: {str(e)}")
                    traceback.print_exc()
                    continue
            
            return results
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la recherche: {str(e)}")
            print("La recherche ne peut pas continuer.")
            traceback.print_exc()
            return []
    
    def _extract_linkedin_links(self):
        """Extrait les liens LinkedIn des r√©sultats de recherche avec plusieurs strat√©gies"""
        linkedin_links = []
        
        # STRAT√âGIE 1: Essayer de trouver tous les r√©sultats avec les s√©lecteurs modernes 2024
        result_selectors = [
            "#search div.MjjYud",  # Nouveau s√©lecteur Google 2024
            "div.g",               # Ancien s√©lecteur Google classique
            "#rso div.g",          # Variante avec parent
            "#center_col div",     # Plus g√©n√©ral
            "[data-hveid]",        # Attribut data sp√©cifique √† Google
            "div.v7W49e > div",    # Structure parfois utilis√©e
            "div.hlcw0c"           # Autre variante
        ]
        
        for selector in result_selectors:
            try:
                results = self.driver.find_elements(By.CSS_SELECTOR, selector)
                print(f"  Tentative avec '{selector}': {len(results)} r√©sultats trouv√©s")
                
                if results:
                    for result in results:
                        # Essayer d'extraire le lien LinkedIn de ce r√©sultat
                        link_info = self._extract_link_from_result(result)
                        if link_info and link_info.get('url'):
                            linkedin_links.append(link_info)
                    
                    if linkedin_links:
                        print(f"  ‚úì {len(linkedin_links)} liens LinkedIn extraits avec le s√©lecteur '{selector}'")
                        break  # Utiliser cette strat√©gie si elle fonctionne
            except Exception as e:
                print(f"  ‚ö†Ô∏è Erreur avec le s√©lecteur '{selector}': {str(e)}")
        
        # STRAT√âGIE 2 (fallback): Chercher directement tous les liens LinkedIn
        if not linkedin_links:
            print("  Utilisation de la strat√©gie de fallback: recherche directe des liens")
            try:
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                for link in all_links:
                    href = link.get_attribute("href")
                    if href and "linkedin.com/in/" in href:
                        # Essayer d'extraire le texte et le parent
                        link_text = link.text
                        parent_text = ""
                        
                        try:
                            # Remonter pour trouver le bloc parent qui contient ce lien
                            parent = link.find_element(By.XPATH, "./ancestor::div[contains(@class, 'g') or contains(@class, 'MjjYud') or @data-hveid]")
                            if parent:
                                parent_text = parent.text
                        except:
                            # Si on ne peut pas trouver le parent, utiliser le texte du lien uniquement
                            pass
                        
                        linkedin_links.append({
                            'url': href,
                            'element': link,
                            'title': link_text,
                            'full_text': parent_text or link_text
                        })
                
                print(f"  ‚úì {len(linkedin_links)} liens LinkedIn trouv√©s par recherche directe")
            except Exception as e:
                print(f"  ‚ö†Ô∏è Erreur lors de la recherche directe: {str(e)}")
        
        # Filtrer les liens uniques
        unique_links = []
        seen_urls = set()
        
        for link in linkedin_links:
            url = link.get('url', '')
            if url and url not in seen_urls and "linkedin.com/in/" in url:
                seen_urls.add(url)
                unique_links.append(link)
        
        print(f"  ‚úì {len(unique_links)} liens LinkedIn uniques trouv√©s au total")
        return unique_links
    
    def _extract_link_from_result(self, result_element):
        """Extrait un lien LinkedIn et son texte √† partir d'un √©l√©ment de r√©sultat"""
        link_info = {'element': result_element, 'full_text': result_element.text}
        
        # Strat√©gie 1: Chercher les liens directs
        try:
            links = result_element.find_elements(By.TAG_NAME, "a")
            for link in links:
                href = link.get_attribute("href")
                if href and "linkedin.com/in/" in href:
                    link_info['url'] = href
                    
                    # Essayer d'extraire le titre de diff√©rentes mani√®res
                    try:
                        # M√©thode 1: h3 dans le lien
                        h3 = link.find_element(By.TAG_NAME, "h3")
                        if h3 and h3.text.strip():
                            link_info['title'] = h3.text.strip()
                    except:
                        pass
                    
                    if not link_info.get('title'):
                        # M√©thode 2: titre du lien
                        link_info['title'] = link.text.strip() or link.get_attribute("title") or ""
                    
                    # Si on a un URL et au moins un texte, c'est suffisant
                    if link_info.get('url') and (link_info.get('title') or link_info.get('full_text')):
                        return link_info
        except:
            pass
        
        # Strat√©gie 2: Chercher les h3 puis remonter aux liens
        try:
            headings = result_element.find_elements(By.TAG_NAME, "h3")
            for heading in headings:
                try:
                    # Remonter au lien parent
                    parent_link = heading.find_element(By.XPATH, "./ancestor::a")
                    href = parent_link.get_attribute("href")
                    if href and "linkedin.com/in/" in href:
                        link_info['url'] = href
                        link_info['title'] = heading.text.strip()
                        return link_info
                except:
                    pass
        except:
            pass
        
        # Si on a trouv√© une URL dans les tentatives pr√©c√©dentes, renvoyer ce qu'on a
        if link_info.get('url'):
            return link_info
        
        # Sinon, renvoyer None pour indiquer qu'aucun lien LinkedIn n'a √©t√© trouv√©
        return None
    
    def _extract_profile_from_result(self, link_info, job_title):
        """Extrait les informations d'un profil √† partir d'un r√©sultat de recherche"""
        profile_info = {
            'Intitul√© de poste': job_title,
            'LinkedIn': link_info.get('url', ''),
            'Pr√©nom': '',
            'Nom': '',
            'Entreprise': '',
            'Date d\'ajout': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
            'Notes': ''
        }
        
        # Collecter tous les textes disponibles
        heading = link_info.get('title', '').strip()
        full_text = link_info.get('full_text', '').strip()
        
        # Si heading est vide mais full_text ne l'est pas, essayer d'extraire un meilleur heading
        if not heading and full_text:
            # Essayer d'extraire la premi√®re ligne ou les premiers caract√®res
            lines = full_text.split('\n')
            if lines:
                heading = lines[0].strip()
        
        # Si le heading est toujours vide, tenter une derni√®re extraction directe
        if not heading and link_info.get('element'):
            try:
                # Rechercher un h3 ou tout autre √©l√©ment de titre
                heading_elements = link_info['element'].find_elements(By.XPATH, 
                    ".//*[self::h3 or contains(@class, 'LC20lb') or contains(@class, 'DKV0Md')]")
                if heading_elements:
                    heading = heading_elements[0].text.strip()
            except:
                pass
        
        # Debug: afficher ce qu'on a trouv√©
        print(f"   üîç Informations trouv√©es:")
        print(f"     URL: {link_info.get('url', 'N/A')}")
        print(f"     Titre: {heading or 'N/A'}")
        print(f"     Texte complet: {full_text[:100] + '...' if len(full_text) > 100 else full_text or 'N/A'}")
        
        # Si on n'a toujours pas de texte exploitable, on ne peut pas continuer
        if not heading and not full_text:
            print("   ‚ùå Pas de texte exploitable trouv√©, impossible d'extraire les informations")
            return profile_info
        
        # Fonction pour valider si un nom est r√©aliste
        def is_realistic_name(name):
            """V√©rifie si un nom semble r√©aliste (pas d'artefact d'URL)"""
            if not name or len(name) < 2 or len(name) > 50:
                return False
            
            # Rejeter les noms avec des chiffres
            if re.search(r'\d', name):
                return False
            
            # Rejeter les suites de lettres trop longues (indices d'URLs)
            words = name.replace('-', ' ').replace("'", ' ').split()
            for word in words:
                if len(word) > 15:  # Mot trop long
                    return False
                # Rejeter les mots avec trop de consonnes cons√©cutives (signe d'artefact)
                consonant_count = 0
                for char in word.lower():
                    if char in 'bcdfghjklmnpqrstvwxyz':
                        consonant_count += 1
                        if consonant_count > 4:  # Plus de 4 consonnes cons√©cutives
                            return False
                    else:
                        consonant_count = 0
            
            # Rejeter les noms qui ressemblent √† des codes/hashs
            name_clean = re.sub(r'[^a-zA-Z]', '', name.lower())
            if len(name_clean) > 8:
                # V√©rifier la diversit√© des caract√®res (√©viter aaaaabbbb par exemple)
                char_counts = {}
                for char in name_clean:
                    char_counts[char] = char_counts.get(char, 0) + 1
                
                # Si un caract√®re repr√©sente plus de 40% du nom, c'est suspect
                max_char_ratio = max(char_counts.values()) / len(name_clean)
                if max_char_ratio > 0.4:
                    return False
            
            # V√©rifier que le nom contient au moins une voyelle
            if not re.search(r'[aeiouAEIOU]', name):
                return False
            
            # Rejeter les suites alphab√©tiques suspectes (qwerty, abcd, etc.)
            name_lower = name_clean.lower()
            suspicious_patterns = [
                'qwerty', 'azerty', 'abcdef', 'abcd', 'defg', 'qazwsx',
                'zxcvbn', 'mnbvcx', 'asdfgh', 'hjkl', 'poiuy', 'lkjh'
            ]
            for pattern in suspicious_patterns:
                if pattern in name_lower:
                    return False
            
            # V√©rifier que ce n'est pas une suite alphab√©tique
            if len(name_clean) >= 6:
                for i in range(len(name_clean) - 5):
                    substring = name_clean[i:i+6]
                    # V√©rifier si c'est une suite cons√©cutive dans l'alphabet
                    is_consecutive = True
                    for j in range(1, len(substring)):
                        if ord(substring[j]) != ord(substring[j-1]) + 1:
                            is_consecutive = False
                            break
                    if is_consecutive:
                        return False
            
            return True
        
        # Fonction pour nettoyer et extraire le nom
        def extract_clean_name(text, is_url_source=False):
            """Extrait et nettoie un nom d'un texte"""
            if not text:
                return ""
            
            # Patterns d'extraction selon la source
            if is_url_source:
                # Pour les URLs, √™tre plus restrictif
                name_patterns = [
                    r"^([a-zA-Z]+(?:-[a-zA-Z]+)*?)(?:-\d|$)",  # Pr√©nom-nom (arr√™t avant chiffres)
                    r"^([a-zA-Z]{2,15})$"  # Nom simple sans tirets ni chiffres
                ]
            else:
                # Pour les titres, plus permissif mais plus pr√©cis
                name_patterns = [
                    # Pattern exact pour "Pr√©nom Nom - LinkedIn"
                    r"^([A-Za-z√Ä-√ø]+(?:\s+[A-Za-z√Ä-√ø\-\']+){1,2})\s*[-‚Äì]\s*LinkedIn",
                    # Pattern exact pour "Pr√©nom Nom: Poste" (PRIORIT√â MAXIMALE)
                    r"^([A-Za-z√Ä-√ø]+(?:-[A-Za-z√Ä-√ø]+)?(?:\s+[A-Za-z√Ä-√ø\-\']+){0,2})\s*:",
                    # Pattern SP√âCIAL pour pr√©noms compos√©s avec tiret + nom + " - " + poste (ex: "Jean-Baptiste Dupont - Poste")
                    r"^([A-Za-z√Ä-√ø]+-[A-Za-z√Ä-√ø]+\s+[A-Za-z√Ä-√ø\-\']+)\s+[-‚Äì]\s+",
                    # Pattern pour "Pr√©nom Nom - Titre" (normal sans pr√©nom compos√©)
                    r"^([A-Za-z√Ä-√ø]+\s+[A-Za-z√Ä-√ø\-\']+)\s*[-‚Äì]\s*[^-‚Äì]+",
                    # Pattern exact pour "Pr√©nom Nom | Poste"  
                    r"^([A-Za-z√Ä-√ø]+(?:-[A-Za-z√Ä-√ø]+)?(?:\s+[A-Za-z√Ä-√ø\-\']+){0,2})\s*\|",
                    # Pattern pour "Pr√©nom Nom, Poste"
                    r"^([A-Za-z√Ä-√ø]+(?:-[A-Za-z√Ä-√ø]+)?(?:\s+[A-Za-z√Ä-√ø\-\']+){0,2})\s*,",
                    # Pattern pour capturer avant mots-cl√©s de poste (avec priorit√© sur noms complets)
                    r"^([A-Za-z√Ä-√ø]+(?:-[A-Za-z√Ä-√ø]+)*(?:\s+[A-Za-z√Ä-√ø\-\']+)*)(?:\s+(?:at|chez|@|de|du|CEO|CTO|VP|Manager|Directeur|Engineer|Developer|Consultant|Analyst))",
                    # Pattern pour nom avec titre (ex: "M. Jean Dupont")
                    r"^(?:M\.|Mme|Mr\.|Mrs\.|Dr\.|Pr\.)\s+([A-Za-z√Ä-√ø]+(?:-[A-Za-z√Ä-√ø]+)*(?:\s+[A-Za-z√Ä-√ø\-\']+)*)",
                    # Pattern pour capturer les 2-3 premiers mots si ils ressemblent √† un nom
                    r"^([A-Za-z√Ä-√ø]+(?:-[A-Za-z√Ä-√ø]+)?(?:\s+[A-Za-z√Ä-√ø\-\']+){1,2})(?:\s|$)",
                    # Pattern pour nom simple en dernier recours (1 mot seulement)
                    r"^([A-Za-z√Ä-√ø]{3,15})(?:\s|$)"
                ]
            
            for pattern in name_patterns:
                match = re.search(pattern, text.strip(), re.IGNORECASE)
                if match:
                    potential_name = match.group(1).strip()
                    # Limiter √† 3 mots maximum pour un nom
                    name_words = potential_name.split()
                    if len(name_words) <= 3 and is_realistic_name(potential_name):
                        return potential_name.title()
            
            return ""
        
        # Essayer d'extraire du titre/heading d'abord (plus fiable)
        clean_name = extract_clean_name(heading, False) if heading else ""
        name_found = bool(clean_name)
        
        # Si pas trouv√© dans le titre mais qu'on a du texte complet, essayer avec celui-ci
        if not name_found and full_text:
            # Essayer les premi√®res lignes du texte complet
            lines = full_text.split('\n')
            for line in lines[:3]:  # Essayer les 3 premi√®res lignes
                if line.strip():
                    clean_name = extract_clean_name(line.strip(), False)
                    if clean_name:
                        name_found = True
                        break
        
        # Si toujours pas trouv√©, essayer l'URL LinkedIn (en dernier recours)
        if not name_found:
            linkedin_url = link_info.get('url', '')
            if linkedin_url:
                linkedin_match = re.search(r"linkedin\.com/in/([a-zA-Z\-]+)", linkedin_url, re.IGNORECASE)
                if linkedin_match:
                    url_part = linkedin_match.group(1)
                    # Nettoyer et reformater le nom depuis l'URL
                    if '-' in url_part and len(url_part) > 3:
                        # S√©parer par tirets et valider chaque partie
                        parts = url_part.split('-')
                        valid_url_parts = []
                        for part in parts:
                            if len(part) >= 2 and part.isalpha() and is_realistic_name(part):
                                valid_url_parts.append(part.capitalize())
                        
                        if len(valid_url_parts) >= 2:
                            # Prendre les 2 premiers comme pr√©nom et nom
                            clean_name = ' '.join(valid_url_parts[:2])
                            name_found = True
                        elif len(valid_url_parts) == 1:
                            # Un seul nom valide, l'utiliser comme pr√©nom
                            clean_name = valid_url_parts[0]
                            name_found = True
                    elif len(url_part) >= 3 and url_part.isalpha() and is_realistic_name(url_part):
                        # URL sans tirets mais nom valide
                        clean_name = url_part.capitalize()
                        name_found = True
        
        # Traitement du nom trouv√©
        if name_found and clean_name:
            name_parts = clean_name.split()
            # Double v√©rification de chaque partie
            valid_parts = []
            for part in name_parts:
                if is_realistic_name(part) and len(part) >= 2:
                    valid_parts.append(part)
            
            if len(valid_parts) >= 1:
                profile_info['Pr√©nom'] = valid_parts[0]
                if len(valid_parts) > 1:
                    profile_info['Nom'] = ' '.join(valid_parts[1:])
                else:
                    profile_info['Nom'] = ''
                print(f"   ‚úì Nom extrait: {profile_info['Pr√©nom']} {profile_info['Nom']}")
            else:
                name_found = False
                print("   ‚ùå Parties du nom invalides")
        else:
            print("   ‚ùå Aucun nom extrait")
        
        # Fonction pour extraire l'entreprise
        def extract_company(text):
            """Extrait le nom de l'entreprise d'un texte"""
            if not text:
                return ""
            
            company_patterns = [
                # Pattern PRIORITAIRE pour "Pr√©nom Nom - Titre - Entreprise" (ex: "K√©vin Moreno - Senior Technical Recruiter - Ubisoft Paris")
                r"^[A-Za-z√Ä-√ø\s\-\']+\s*[-‚Äì]\s*[^-‚Äì]+\s*[-‚Äì]\s*([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+?)(?:\s*$|\s*[-|‚Ä¢])",
                # Patterns sp√©cifiques avec caract√®res sp√©ciaux fran√ßais
                r"^[A-Za-z√Ä-√ø\s\-\']+\s*[-‚Äì]\s*[^-‚Äì]+\s*[-‚Äì]\s*([LlDd]['\'][A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+?)(?:\s*$|\s*[-|‚Ä¢])",
                # Patterns avec | pour les titres LinkedIn
                r"\|\s*([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+?)(?:\s*$|\s*[-|‚Ä¢])",
                # Patterns fran√ßais
                r"(?:chez|Chez)\s+([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+?)(?:\s*[-|‚Ä¢]|\s*$|\s*\|)",
                r"(?:√†|√Ä)\s+([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+?)(?:\s*[-|‚Ä¢]|\s*$|\s*\|)",
                r"(?:at|At)\s+([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+?)(?:\s*[-|‚Ä¢]|\s*$|\s*\|)",
                r"(?:@)\s*([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+?)(?:\s*[-|‚Ä¢]|\s*$|\s*\|)",
                # Patterns avec tirets (apr√®s nom et titre)
                r"[-‚Äì]\s*([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+?)(?:\s*[-|‚Ä¢]|\s*$|\s*\|)",
                # Patterns pour les titres LinkedIn typiques
                r"(?:Director|Manager|Engineer|Analyst|Consultant|Developer|Chef|Responsable|Directeur|Ing√©nieur|D√©veloppeur)\s+(?:at|chez)\s+([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+)",
                # Pattern pour capturer apr√®s le nom et titre
                r"(?:CEO|CTO|VP|President|Pr√©sident|PDG|DG|G√©rant)\s+(?:at|chez|de|d')\s*([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+)",
                # Patterns pour description
                r"travaille\s+(?:chez|pour)\s+([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+)",
                r"works\s+(?:at|for)\s+([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+)",
                # Pattern g√©n√©ral pour mots capitalis√©s apr√®s certains mots-cl√©s
                r"(?:Company|Soci√©t√©|Entreprise|Groupe|Group)[:,\s]+([A-Za-z√Ä-√ø][a-zA-Z0-9√Ä-√ø\s&\-\.\']+)"
            ]
            
            for pattern in company_patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    company = match.group(1).strip()
                    # Nettoyer le nom de l'entreprise
                    company = re.sub(r'\s+', ' ', company)
                    company = re.sub(r'^(le|la|les|un|une|des|du|de|d\')\s+', '', company, flags=re.IGNORECASE)
                    company = company.strip(' -.,|‚Ä¢')
                    
                    # V√©rifications de qualit√©
                    if (len(company) > 2 and len(company) < 80 and 
                        not re.match(r'^[0-9]+$', company) and
                        not company.lower() in ['linkedin', 'profile', 'profil', 'france', 'paris', 'voir', 'plus'] and
                        not re.match(r'^(voir|see|view|plus|more)', company.lower())):
                        
                        return company
            
            return ""
        
        # Essayer d'extraire l'entreprise √† partir du titre et du texte complet
        company = ""
        if heading:
            company = extract_company(heading)
        
        if not company and full_text:
            # Essayer les premi√®res lignes du texte complet
            lines = full_text.split('\n')
            for line in lines[:3]:  # Essayer les 3 premi√®res lignes
                if line.strip():
                    company = extract_company(line.strip())
                    if company:
                        break
        
        # Si l'entreprise a √©t√© trouv√©e, l'ajouter au profil
        if company:
            profile_info['Entreprise'] = company
            print(f"   ‚úì Entreprise extraite: {company}")
        else:
            print("   ‚ùå Aucune entreprise extraite")
        
        return profile_info
    
    def add_profile_to_data(self, profile_info):
        """Ajoute un nouveau profil aux donn√©es existantes en √©vitant les doublons"""
        
        # V√©rifier que le profil a les informations essentielles
        if not profile_info.get('Pr√©nom') or not profile_info.get('Nom') or not profile_info.get('Entreprise'):
            safe_print(f"  ‚ùå Profil incomplet, non ajout√©: informations manquantes")
            return False
        
        # V√©rifier si le profil existe d√©j√† (bas√© sur LinkedIn URL)
        if 'LinkedIn' in profile_info and profile_info['LinkedIn']:
            existing = self.data[self.data['LinkedIn'] == profile_info['LinkedIn']]
            if not existing.empty:
                print(f"  ‚ö†Ô∏è  Profil d√©j√† existant: {profile_info.get('Pr√©nom', '')} {profile_info.get('Nom', '')} - Ignor√©")
                return False
        
        # V√©rifier les doublons bas√©s sur nom + entreprise
        if profile_info.get('Pr√©nom') and profile_info.get('Nom') and profile_info.get('Entreprise'):
            existing = self.data[
                (self.data['Pr√©nom'] == profile_info['Pr√©nom']) & 
                (self.data['Nom'] == profile_info['Nom']) & 
                (self.data['Entreprise'] == profile_info['Entreprise'])
            ]
            if not existing.empty:
                print(f"  ‚ö†Ô∏è  Profil similaire d√©j√† existant: {profile_info.get('Pr√©nom', '')} {profile_info.get('Nom', '')} - Ignor√©")
                return False
        
        # Ajouter le profil aux donn√©es
        new_row = pd.DataFrame([profile_info])
        self.data = pd.concat([self.data, new_row], ignore_index=True)
        safe_print(f"  ‚úì Profil ajout√©: {profile_info.get('Pr√©nom', '')} {profile_info.get('Nom', '')} chez {profile_info.get('Entreprise', '')}")
        safe_print(f"    LinkedIn: {profile_info.get('LinkedIn', '')}")
        return True
    
    def search_by_job_title(self, job_title, max_results=5):
        """Recherche des profils bas√©s sur un intitul√© de poste et les ajoute aux donn√©es existantes"""
        safe_print(f"\nRecherche de profils pour: {job_title}")
        safe_print(f"Nombre de profils avant recherche: {len(self.data)}")
        
        # Recherche sur LinkedIn avec techniques anti-CAPTCHA
        profiles = self.search_linkedin_with_delays(job_title, max_results)
        
        safe_print(f"Profils trouv√©s par la recherche: {len(profiles)}")
        
        if not profiles:
            safe_print("Aucun profil trouv√© par la recherche!")
            safe_print("La recherche n'a pas pu trouver de profils correspondants.")
        
        # Ajouter la date d'ajout √† chaque profil
        for profile in profiles:
            if 'Date d\'ajout' not in profile:
                profile['Date d\'ajout'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
            if 'Notes' not in profile:
                profile['Notes'] = ''
            safe_print(f"   Profil trouv√©: {profile.get('Pr√©nom', '')} {profile.get('Nom', '')} chez {profile.get('Entreprise', '')}")
        
        # Ajouter les r√©sultats aux donn√©es existantes en √©vitant les doublons
        added_count = 0
        for profile in profiles:
            if self.add_profile_to_data(profile):
                added_count += 1
        
        safe_print(f"{added_count} nouveaux profils ajout√©s pour '{job_title}' (sur {len(profiles)} trouv√©s)")
        safe_print(f"Total de profils maintenant: {len(self.data)}")
        
        return profiles
    
    def save_to_excel(self, output_file="Resultats_Profils.xlsx"):
        """Sauvegarde les donn√©es dans un fichier Excel"""
        try:
            safe_print(f"\nSAUVEGARDE EN COURS...")
            safe_print(f"Fichier de destination: {output_file}")
            safe_print(f"Nombre de lignes dans self.data: {len(self.data)}")
            
            if self.data.empty:
                safe_print("ATTENTION: Aucune donn√©e √† sauvegarder!")
                safe_print("   Le DataFrame est vide. Cr√©ation d'un fichier vide...")
            else:
                safe_print(f"Donn√©es √† sauvegarder:")
                safe_print(f"   Colonnes: {list(self.data.columns)}")
                safe_print(f"   Premiers profils:")
                for i, row in self.data.head(3).iterrows():
                    safe_print(f"   {i+1}. {row.get('Pr√©nom', '')} {row.get('Nom', '')} - {row.get('Intitul√© de poste', '')}")
            
            # Cr√©er le r√©pertoire s'il n'existe pas
            os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)
            
            # Sauvegarder le fichier
            self.data.to_excel(output_file, index=False)
            safe_print(f"Donn√©es sauvegard√©es dans {output_file}")
            safe_print(f"Total de profils: {len(self.data)}")
            
            # V√©rifier que le fichier a bien √©t√© cr√©√©
            if os.path.exists(output_file):
                file_size = os.path.getsize(output_file)
                safe_print(f"Fichier cr√©√© - Taille: {file_size} bytes")
            else:
                safe_print("ERREUR: Fichier non cr√©√©!")
                return False
            
            # Afficher un r√©sum√© des donn√©es
            if not self.data.empty:
                safe_print(f"R√©partition par poste:")
                postes_count = self.data['Intitul√© de poste'].value_counts()
                for poste, count in postes_count.head(5).items():
                    safe_print(f"   ‚Ä¢ {poste}: {count} profils")
            
            return True
        except Exception as e:
            safe_print(f"Erreur lors de la sauvegarde du fichier Excel: {str(e)}")
            import traceback
            safe_print(f"D√©tails de l'erreur:")
            traceback.print_exc()
            return False
    
    def close(self):
        """Ferme le navigateur"""
        if self.driver:
            self.driver.quit()
            print("Navigateur ferm√©")

def main():
    # Correction pour la gestion des arguments
    parser = argparse.ArgumentParser(description="Scraper de profils professionnels bas√© sur des intitul√©s de postes")
    parser.add_argument("--input", "-i", help="Fichier Excel contenant les donn√©es existantes")
    parser.add_argument("--output", "-o", default="Resultats_Profils.xlsx", help="Fichier Excel de sortie (sera ajout√© au fichier existant)")
    parser.add_argument("--job", "-j", help="Intitul√© de poste √† rechercher")
    parser.add_argument("--count", "-c", default=5, help="Nombre de r√©sultats √† r√©cup√©rer par intitul√© de poste")
    parser.add_argument("--slow", action="store_true", help="Mode lent avec d√©lais suppl√©mentaires pour √©viter les CAPTCHA")
    
    args = parser.parse_args()
    
    # Traitement s√©curis√© du nombre de r√©sultats
    try:
        count = int(args.count)
        if count <= 0:
            count = 5
    except (ValueError, TypeError):
        print(f"ATTENTION: Valeur incorrecte pour --count: '{args.count}'. Utilisation de la valeur par d√©faut: 5")
        count = 5
    
    scraper = ProfileScraper(args.input, args.slow)
    
    try:
        if args.job:
            # Recherche pour un intitul√© de poste sp√©cifique
            scraper.search_by_job_title(args.job, count)
        elif scraper.data is not None and 'Intitul√© de poste' in scraper.data.columns:
            # Recherche pour tous les intitul√©s de postes du fichier d'entr√©e
            job_titles = scraper.data['Intitul√© de poste'].unique()
            for job in job_titles:
                if pd.notna(job) and job.strip():
                    scraper.search_by_job_title(job, count)
        else:
            print("Veuillez sp√©cifier un intitul√© de poste avec --job ou fournir un fichier Excel valide avec --input")
            return
        
        # Sauvegarde des r√©sultats
        scraper.save_to_excel(args.output)
    
    finally:
        scraper.close()

if __name__ == "__main__":
    main()
